import pandas as pd
import os
import json
import ollama
from langchain_core.tools import tool



def get_static_data_df(file_paths):

    all_data = []
    for file_path in file_paths:
        try:
            _, file_extension = os.path.splitext(file_path)

            if file_extension.lower() == ".csv":
                df = pd.read_csv(file_path)
                all_data.append(df.to_string())  # convert to string

            elif file_extension.lower() == ".json":
                with open(file_path, 'r') as f:
                    data = json.load(f)
                all_data.append(str(data))  # convert to string

            elif file_extension.lower() == ".txt":
                with open(file_path, 'r') as f:
                    data = f.read()
                all_data.append(data)

            else:
                return f"Unsupported file format: {file_extension}"

        except FileNotFoundError:
            return f"File not found: {file_path}"
        except Exception as e:
            return f"Error reading file {file_path}: {str(e)}"

            # Combine all data into a single string for the LLM
    combined_data = "\n".join(all_data)
    return combined_data



@tool
def query_dataframe_static_data(user_message):

    """
    Retrieves static data related to Configurable Items, server-application dependencies, and incidents.

    This tool fetches data from predefined CSV and JSON files, representing configuration management database (CMDB) information,
    application dependency graphs, and incident reports. It then uses an LLM to answer user queries based on this static data.

    Args:
        user_query (str): The user's question or request for information about configurable items,
                         server-application dependencies, or incidents.

    Returns:
        str: A concise, single-sentence answer generated by the LLM based on the static data.
             Returns an error message if the query cannot be processed.

    Example:
        User Input: "What applications run on server SVR-001?"
        Tool Output: "Static Data: Application APP-001 and APP-002 run on server SVR-001."

        User Input: "List all open incidents."
        Tool Output: "Static Data: Incident INC-003 and INC-005 are currently open."

        User Input: "Show all configurable items."
        Tool Output: "Static Data: Configurable items are routers, servers, and databases."

        User Input: "Is there a problem with the database?"
        Tool Output: "Static Data: There is no problem with the database."
    """
    df_str = get_static_data_df(['../data/cmdb_data.csv', '../data/dependency_graph.json', '../data/incident_data.csv'])

    prompt = f"""
            You are very efficient Platform Support Assistant, you are given data: {df_str}. infer and respond to this prompt in single human understandable sentence: {user_message} . Do not give Explainations and do not give any Queries.
    """

    try:

        output = ollama.generate(
            #    model="deepseek-r1:7b",
            model='llama3.2',
            #    prompt=f"Using this data: {data}. Respond to this prompt: {input}"
            prompt=prompt
        )

        return output.response

    except Exception as e:
        return f"An error occurred: {e}"

print(query_dataframe_static_data("Can you tell me about Incident 'INC-00024'"))







